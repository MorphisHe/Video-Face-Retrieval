{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_project_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Using Pre-trained ResNet50 Network\n",
        "\n",
        "This is the baseline model that is used in the given [link](https://aws.amazon.com/blogs/machine-learning/building-a-visual-search-application-with-amazon-sagemaker-and-amazon-es/). \n",
        "\n",
        "We are getting the LFW dataset using sklearn package, it provides \"interesting\" (or face area) part of the images directly. "
      ],
      "metadata": {
        "id": "GCThaRlKhbWP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yQ4HXNnor1fz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86008fdd-3d87-4677-8f85-ecc0f24b70a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "94781440/94765736 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "import numpy as np\n",
        "\n",
        "lfw = fetch_lfw_people(color=True, resize=1)\n",
        "images = preprocess_input(lfw['images'])\n",
        "labels = lfw['target']\n",
        "\n",
        "model = ResNet50(weights='imagenet', include_top=False, input_shape=(125, 94, 3), pooling='avg')\n",
        "vectors = model.predict(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using KNN for Vector Search\n",
        "\n",
        "We are using the NearestNeighbors class from sklearn to generate neighbours for given image queries. "
      ],
      "metadata": {
        "id": "TyhuXDVLhlNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "knn = NearestNeighbors(n_jobs=-1)\n",
        "knn.fit(vectors)\n",
        "neigh = knn.kneighbors(vectors, 20, return_distance=False)"
      ],
      "metadata": {
        "id": "31kC7hIVbAxX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Search Results\n",
        "\n",
        "This part of evaluation is not required by the project, however, we come up with a simple evaluation algorithm since we want to compare the performance of our improved model to our baseline model. The algorithm used will be introduced here as well as the same part in the notebook of our improved model. \n",
        "\n",
        "The idea behind our evaluation algorithm is to find the images of same persons in the dataset. If there are three images of Elon Musk in our dataset, we set one of his image as the input query image, and we can find both other two images of him in the neighbours generated by our model, then we will say our model achieves 100% accuracy. Same process as above, if we only get one of the other two images of Elon Musk, we will say out model achieves 50% of accuracy. For those people with only one image in the dataset, we ignore them when computing accuracy but we do not ignore their images during computing accuracy for other people. \n",
        "\n",
        "We can find the accuracy of the baseline model is not that good, we cannot find a lot of same person iamges. "
      ],
      "metadata": {
        "id": "4mQGu6-niGg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5749 people in total\n",
        "faces_per_person = np.zeros(5749, dtype=int)\n",
        "\n",
        "for label in labels:\n",
        "  faces_per_person[label] += 1\n",
        "\n",
        "acc = 0\n",
        "for n in neigh:\n",
        "  faces_cur_person = faces_per_person[labels[n[0]]]\n",
        "\n",
        "  # we don't want person with only 1 image during evaluation\n",
        "  # but we can use them as noise so skip them\n",
        "  if faces_cur_person == 1:\n",
        "    continue\n",
        "\n",
        "  # we calculate 20 nearest neighbours, so 20 as maximum\n",
        "  if faces_cur_person > 20:\n",
        "    faces_cur_person = 20\n",
        "  \n",
        "  cnt = 0\n",
        "  for i in range(20):\n",
        "    if labels[n[i]] == labels[n[0]]:\n",
        "      cnt += 1\n",
        "\n",
        "  acc += (cnt-1)/(faces_cur_person-1)\n",
        "\n",
        "# 9164 images of 1680 people with 2 or more images\n",
        "print(acc/9164)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SLckcA9fstA",
        "outputId": "166b9d19-6b6f-4637-8fa6-39169184ffa3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0979594586947156\n"
          ]
        }
      ]
    }
  ]
}